<?xml version="1.0" encoding="UTF-8"?>
<actions>
    
    <action>
        <actionName>CUSTOM-clean</actionName>
        <displayName>clean</displayName>
        <goals>
            <goal>clean</goal>
        </goals>
    </action>
    
    <action>
        <actionName>CUSTOM-clean_package</actionName>
        <displayName>clean_package</displayName>
        <goals>
            <goal>clean</goal>
            <goal>package</goal>
        </goals>
    </action>
    
    <action>
        <actionName>CUSTOM-cluster_package</actionName>
        <displayName>cluster_package</displayName>
        <activatedProfiles>
            <activatedProfile>spark-cluster</activatedProfile>
        </activatedProfiles>
        <goals>
            <goal>clean</goal>
            <goal>package</goal>
        </goals>
    </action>
    
    <action>
        <actionName>CUSTOM-spark-local</actionName>
        <displayName>spark-local</displayName>
        <goals>
            <goal>process-classes</goal>
            <goal>org.codehaus.mojo:exec-maven-plugin:3.0.0:exec</goal>
        </goals>
        <properties>
            <exec.workingDirectory>${basedir}</exec.workingDirectory>
            <exec.executable>${basedir}/spark-run.cmd</exec.executable>
            <exec.args>'${packageClassName}' 'file:///E:/apps/hostpath/spark/' 'file:///E:/apps/hostpath/spark/out'</exec.args>
            <Env.EXE_TYPE>LOCAL</Env.EXE_TYPE>
            <Env.SPARK_HOME>E:\apps\spark-3.0.1-hadoop3.2.0</Env.SPARK_HOME>
            <Env.HADOOP_HOME>E:\apps\winutils-master\hadoop-3.2.0</Env.HADOOP_HOME>
        </properties>
    </action>
    
    <action>
        <actionName>CUSTOM-spark-docker</actionName>
        <displayName>spark-docker</displayName>
        <activatedProfiles>
            <activatedProfile>spark-cluster</activatedProfile>
        </activatedProfiles>
        <goals>
            <goal>process-classes</goal>
            <goal>org.codehaus.mojo:exec-maven-plugin:3.0.0:exec</goal>
        </goals>
        <properties>
            <exec.workingDirectory>${basedir}</exec.workingDirectory>
            <exec.executable>${basedir}/spark-run.cmd</exec.executable>
            <exec.args>'${packageClassName}' 'hdfs://namenode:9000/files/' 'hdfs://namenode:9000/out'</exec.args>
            <Env.EXE_TYPE>DOCKER</Env.EXE_TYPE>
        </properties>
    </action>
    
    <action>
        <actionName>run.single.main</actionName>
        <preAction>build-with-dependencies</preAction>
        <packagings>
            <packaging>*</packaging>
        </packagings>
        <goals>
            <goal>process-classes</goal>
            <goal>org.codehaus.mojo:exec-maven-plugin:3.0.0:exec</goal>
        </goals>
        <properties>
            <exec.args>-classpath %classpath ${packageClassName} 'file:///E:/apps/hostpath/spark/' 'file:///E:/apps/hostpath/spark/out'</exec.args>
            <exec.executable>java</exec.executable>
            <exec.classpathScope>${classPathScope}</exec.classpathScope>
        </properties>
    </action>
    
    <action>
        <actionName>debug.single.main</actionName>
        <packagings>
            <packaging>*</packaging>
        </packagings>
        <goals>
            <goal>process-classes</goal>
            <goal>org.codehaus.mojo:exec-maven-plugin:3.0.0:exec</goal>
        </goals>
        <properties>
            <exec.args>-agentlib:jdwp=transport=dt_socket,server=n,address=${jpda.address} -classpath %classpath ${packageClassName} 'file:///E:/apps/hostpath/spark/' 'file:///E:/apps/hostpath/spark/out'</exec.args>
            <exec.executable>java</exec.executable>
            <exec.classpathScope>${classPathScope}</exec.classpathScope>
            <jpda.listen>true</jpda.listen>
        </properties>
    </action>
    
</actions>
